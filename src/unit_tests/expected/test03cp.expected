** ====================================================================================== 
**
**             ___________                          _______
**     _______/          /_____ ________ __ _   ___/       \ ____
**    /      /___    ___/      /__   __/  /  \ /  /\__ _   /     \
**   /  //  /   /   /  /  //  /  /  / /  /    /  /___/_   /  //  /
**  /  ___ /   /   /  /  _   /  /  / /  /  /    //       /  //  /
** /__/       /___/  /__//__/  /__/ /__/__/ \__//_______/______/
**
** Authors:  Dave A. May          (dave.may@erdw.ethz.ch)           
**           Laetitia Le Pourhiet (laetitia.le_pourhiet@upmc.fr)    
**           Jed Brown            (jedbrown@mcs.anl.gov)            
**
** git url: https://bitbucket.org/jedbrown/ptatin3d.git 
** commit hash: [out-of-date] Execute "make releaseinfo" to update to the most recent revision 
** log: [out-of-date] Execute "make releaseinfo" to update to the most recent revision 
**                                                                       
** TATIN_CFLAGS = -std=gnu99 -O0 -g -Wall -Wno-unused-variable
**                                                                       
** WARNING pTatin3d appears to have been compiled with debug options 
** For significant performance improvements, please consult the file makefile.arch  
** Adjust TATIN_CFLAGS to include aggressive compiler optimizations 
**                                                                       
** ====================================================================================== 
RheologyConstantsInitialise: global viscosity cut-off, min= 1.000000e-100, max = 1.000000e+100  
[pTatin] Writing output to existing directory: pt3dout 
[pTatin] Created log file: pt3dout/ptatin.log-2015.07.10_15:02:25 
[pTatin] Writing output to existing directory: pt3dout 
[pTatin] Created options file: pt3dout/ptatin.options-2015.07.10_15:02:25 
[pTatin] Created options file: pt3dout/ptatin.options 
  [pTatinModel]: Registering model [0] with name "template"
  [pTatinModel]: Registering model [1] with name "viscous_sinker"
  [pTatinModel]: Registering model [2] with name "Gene3D"
  [pTatinModel]: Registering model [3] with name "Gene3DNueve"
  [pTatinModel]: Registering model [4] with name "indentor"
  [pTatinModel]: Registering model [5] with name "rift3D"
  [pTatinModel]: Registering model [6] with name "rift3D_T"
  [pTatinModel]: Registering model [7] with name "sierra"
  [pTatinModel]: Registering model [8] with name "advdiff_example"
  [pTatinModel]: Registering model [9] with name "delamination"
  [pTatinModel]: Registering model [10] with name "Riftrh"
  [pTatinModel]: Registering model [11] with name "geomod2008"
  [pTatinModel]: Registering model [12] with name "multilayer_folding"
  [pTatinModel]: Registering model [13] with name "submarinelavaflow"
  [pTatinModel]: Registering model [14] with name "ex_subduction"
  [pTatinModel]: Registering model [15] with name "iplus"
  [pTatinModel]: Registering model [16] with name "subduction_initiation2d"
  [pTatinModel]: Registering model [17] with name "convection2d"
  [pTatinModel]: Registering model [18] with name "thermal_sb"
  [pTatinModel]: Registering model [19] with name "sd3d"
  [pTatinModel]: Registering model [20] with name "pas"
  [pTatinModel]: Registering model [21] with name "pd"
  [pTatinModel]: -ptatin_model "viscous_sinker" was detected
pTatin3dRestart: Required to specify a suffix for your restart file via -restart_prefix
pTatin3dRestart: Unable to restart job
[[ModelInitialize_ViscousSinker]]
BCList: Mem. usage (min,max) = 4.56e-01,5.42e-01 (MB) 
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 110592 
  buffer (max)       = 8 
  allocated          = 110600 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 6.64e-01 (MB) : rank0
  Total mem. usage                                                      = 5.31e+00 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 0]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 1]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 2]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 3]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 4]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 2.24e-04 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
DataBucketView(MPI): ("SurfaceGaussLegendre StokesCoefficients[face 5]")
  L                  = 2304 
  buffer (max)       = 8 
  allocated          = 2316 
  nfields registered = 1 
    [  0]: field name  ==>>             QPntSurfCoefStokes : Mem. usage = 6.46e-02 (MB) : rank0
  Total mem. usage                                                      = 2.59e-01 (MB) : collective
  MaterialPointsStokes: Using Q1 projection
[[Swarm initialization: 0.0004 (sec)]]
SwarmMPntStd_AssignUniquePointIdentifiers : max_pid = 0 
[[Swarm->coordinate assignment: 4096 points : 0.0031 (sec)]]
************************** Starting _DataExCompleteCommunicationMap ************************** 
max_nnz = 7 
Mat Object: 8 MPI processes
  type: mpiaij
  rows=8, cols=8
  total: nonzeros=56, allocated nonzeros=64
  total number of mallocs used during MatSetValues calls =0
    not using I-node (on process 0) routines
************************** Ending _DataExCompleteCommunicationMap [setup time: 4.5999e-02 (sec)] ************************** 
[[SwarmDMDA3dDataExchangerCreate: time = 4.6490e-02 (sec)]]
[[ModelApplyInitialMeshGeometry_ViscousSinker]]
RUNNING DEFORMED MESH EXAMPLE 
[[ViscousSinker_ApplyInitialMaterialGeometry_SingleInclusion]]
[[ModelApplyBoundaryCondition_ViscousSinker]]
Mesh size (16 x 16 x 16) : MG levels 3  
         level [ 0]: global Q2 elements (2 x 8 x 2) 
         level [ 1]: global Q2 elements (4 x 16 x 4) 
         level [ 2]: global Q2 elements (16 x 16 x 16) 
[r   0]: level [ 0]: local Q2 elements  (1 x 4 x 1) 
[r   0]: level [ 1]: local Q2 elements  (2 x 8 x 2) 
[r   0]: level [ 2]: local Q2 elements  (8 x 8 x 8) 
[r   0]: level [ 0]: element range [0 - 0] x [0 - 3] x [0 - 0] 
[r   0]: level [ 1]: element range [0 - 1] x [0 - 7] x [0 - 1] 
[r   0]: level [ 2]: element range [0 - 7] x [0 - 7] x [0 - 7] 
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 864 
  buffer (max)       = 8 
  allocated          = 872 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 5.23e-03 (MB) : rank0
  Total mem. usage                                                      = 4.19e-02 (MB) : collective
VolumeQuadratureCreate_GaussLegendreStokes:
	Using 3x3 pnt Gauss Legendre quadrature
DataBucketView(MPI): ("GaussLegendre StokesCoefficients")
  L                  = 6912 
  buffer (max)       = 8 
  allocated          = 6920 
  nfields registered = 1 
    [  0]: field name  ==>>              QPntVolCoefStokes : Mem. usage = 4.15e-02 (MB) : rank0
  Total mem. usage                                                      = 3.32e-01 (MB) : collective
BCList: Mem. usage (min,max) = 7.68e-03,1.48e-02 (MB) 
BCList: Mem. usage (min,max) = 3.87e-02,5.90e-02 (MB) 
[[ModelApplyBoundaryConditionMG_ViscousSinker]]
[[ModelInitialCondition_ViscousSinker]]
*** Rheology update for RHEOLOGY_VISCOUS selected ***
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.23e-03 (sec)
Level [2]: Coarse grid type :: Re-discretisation :: matrix free operator 
Level [1]: Coarse grid type :: Re-discretisation :: assembled operator 
Level [0]: Coarse grid type :: Galerkin :: assembled operator 
[[ModelOutput_ViscousSinker]]
  writing pvdfilename pt3dout/timeseries_vp.pvd 
[[DESIGN FLAW]] pTatin3d_ModelOutput_VelocityPressure_Stokes: require better physics modularity to extract (u,p) <---| (X) 
[[DESIGN FLAW]] pTatinOutputMeshVelocityPressureVTS_v0_binary: only printing P0 component of pressure field 
pTatin3d_ModelOutput_VelocityPressure_Stokes() -> icbc_vp.(pvd,pvts,vts): CPU time 5.68e-02 (sec) 
  writing pvdfilename pt3dout/timeseries_mpoints_std.pvd 
pTatin3d_ModelOutput_MPntStd() -> icbc_mpoints_std.(pvd,pvtu,vtu): CPU time 4.53e-02 (sec) 
   [[ COMPUTING FLOW FIELD FOR STEP : 0 ]]
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.36e-03 (sec)
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 9.65e-04 (sec)
    0 KSP Residual norm 0.00136185 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1 
      1 KSP Residual norm 0.999355 
      2 KSP Residual norm 0.9669 
    1 KSP Residual norm 0.00134804 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.760022 
      1 KSP Residual norm 0.208088 
      2 KSP Residual norm 0.143028 
    2 KSP Residual norm 0.00134164 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.69234 
      1 KSP Residual norm 0.306639 
      2 KSP Residual norm 0.140158 
    3 KSP Residual norm 0.00132866 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.65839 
      1 KSP Residual norm 0.347484 
      2 KSP Residual norm 0.162666 
    4 KSP Residual norm 0.00132666 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.09325 
      1 KSP Residual norm 0.161418 
      2 KSP Residual norm 0.0604564 
    5 KSP Residual norm 0.00132384 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.71566 
      1 KSP Residual norm 0.247929 
      2 KSP Residual norm 0.0999008 
    6 KSP Residual norm 0.00132384 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.70666 
      1 KSP Residual norm 0.316397 
      2 KSP Residual norm 0.102452 
    7 KSP Residual norm 0.00131862 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.29917 
      1 KSP Residual norm 0.310301 
      2 KSP Residual norm 0.0621086 
    8 KSP Residual norm 0.00128301 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.51798 
      1 KSP Residual norm 0.246656 
      2 KSP Residual norm 0.0747797 
    9 KSP Residual norm 0.00125205 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.00019 
      1 KSP Residual norm 0.23784 
      2 KSP Residual norm 0.0803347 
   10 KSP Residual norm 0.00112805 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.85345 
      1 KSP Residual norm 0.209245 
      2 KSP Residual norm 0.0555516 
   11 KSP Residual norm 0.000924491 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.55811 
      1 KSP Residual norm 0.289613 
      2 KSP Residual norm 0.107052 
   12 KSP Residual norm 0.000659039 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.81937 
      1 KSP Residual norm 0.236963 
      2 KSP Residual norm 0.102945 
   13 KSP Residual norm 0.0005199 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 2.23862 
      1 KSP Residual norm 0.300083 
      2 KSP Residual norm 0.0739895 
   14 KSP Residual norm 0.000364608 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.68814 
      1 KSP Residual norm 0.196581 
      2 KSP Residual norm 0.114131 
   15 KSP Residual norm 0.000214811 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.47541 
      1 KSP Residual norm 0.161484 
      2 KSP Residual norm 0.0956934 
   16 KSP Residual norm 0.000122936 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.65034 
      1 KSP Residual norm 0.30233 
      2 KSP Residual norm 0.159957 
   17 KSP Residual norm 8.89987e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.83258 
      1 KSP Residual norm 0.333129 
      2 KSP Residual norm 0.18782 
   18 KSP Residual norm 7.59343e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.16745 
      1 KSP Residual norm 0.166439 
      2 KSP Residual norm 0.1499 
   19 KSP Residual norm 7.57288e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.32295 
      1 KSP Residual norm 0.104974 
      2 KSP Residual norm 0.0472976 
   20 KSP Residual norm 7.57207e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.18405 
      1 KSP Residual norm 0.145451 
      2 KSP Residual norm 0.0557905 
   21 KSP Residual norm 7.55544e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.923758 
      1 KSP Residual norm 0.100268 
      2 KSP Residual norm 0.0618765 
   22 KSP Residual norm 7.1275e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.27563 
      1 KSP Residual norm 0.0983995 
      2 KSP Residual norm 0.0437413 
   23 KSP Residual norm 4.94206e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.23288 
      1 KSP Residual norm 0.176306 
      2 KSP Residual norm 0.112324 
   24 KSP Residual norm 2.59974e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.06183 
      1 KSP Residual norm 0.262535 
      2 KSP Residual norm 0.242785 
   25 KSP Residual norm 1.7726e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.1986 
      1 KSP Residual norm 0.297319 
      2 KSP Residual norm 0.268183 
   26 KSP Residual norm 1.33925e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.02622 
      1 KSP Residual norm 0.234103 
      2 KSP Residual norm 0.220957 
   27 KSP Residual norm 1.14673e-05 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.46067 
      1 KSP Residual norm 0.171989 
      2 KSP Residual norm 0.12643 
   28 KSP Residual norm 8.35016e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.08502 
      1 KSP Residual norm 0.213294 
      2 KSP Residual norm 0.206263 
   29 KSP Residual norm 5.69551e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.32764 
      1 KSP Residual norm 0.37143 
      2 KSP Residual norm 0.324284 
   30 KSP Residual norm 4.24356e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.787595 
      1 KSP Residual norm 0.615105 
      2 KSP Residual norm 0.605516 
   31 KSP Residual norm 4.18757e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.461205 
      1 KSP Residual norm 0.105584 
      2 KSP Residual norm 0.0861982 
   32 KSP Residual norm 4.16188e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.17421 
      1 KSP Residual norm 0.190819 
      2 KSP Residual norm 0.101237 
   33 KSP Residual norm 4.14475e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.33057 
      1 KSP Residual norm 0.153827 
      2 KSP Residual norm 0.0476084 
   34 KSP Residual norm 4.11773e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.94559 
      1 KSP Residual norm 0.0703459 
      2 KSP Residual norm 0.0310559 
   35 KSP Residual norm 3.96021e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.07047 
      1 KSP Residual norm 0.226314 
      2 KSP Residual norm 0.0959757 
   36 KSP Residual norm 3.76597e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.41068 
      1 KSP Residual norm 0.204239 
      2 KSP Residual norm 0.0775191 
   37 KSP Residual norm 3.35368e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.95356 
      1 KSP Residual norm 0.247119 
      2 KSP Residual norm 0.113699 
   38 KSP Residual norm 2.63842e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.27074 
      1 KSP Residual norm 0.233129 
      2 KSP Residual norm 0.214828 
   39 KSP Residual norm 2.34321e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.20681 
      1 KSP Residual norm 0.192995 
      2 KSP Residual norm 0.131509 
   40 KSP Residual norm 2.33917e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.85345 
      1 KSP Residual norm 0.313132 
      2 KSP Residual norm 0.0979888 
   41 KSP Residual norm 2.27991e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.7281 
      1 KSP Residual norm 0.27449 
      2 KSP Residual norm 0.164804 
   42 KSP Residual norm 2.23791e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.21782 
      1 KSP Residual norm 0.169645 
      2 KSP Residual norm 0.129912 
   43 KSP Residual norm 2.21786e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.3971 
      1 KSP Residual norm 0.173034 
      2 KSP Residual norm 0.102927 
   44 KSP Residual norm 2.21641e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.04668 
      1 KSP Residual norm 0.187899 
      2 KSP Residual norm 0.107428 
   45 KSP Residual norm 2.05817e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 1.04614 
      1 KSP Residual norm 0.15225 
      2 KSP Residual norm 0.129869 
   46 KSP Residual norm 1.67125e-06 
      Residual norms for fieldsplit_u_ solve.
      0 KSP Residual norm 0.925699 
      1 KSP Residual norm 0.196818 
      2 KSP Residual norm 0.165977 
   47 KSP Residual norm 1.06653e-06 
  Linear solve converged due to CONVERGED_RTOL iterations 47
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 4.03e-02 (sec)
SNES Object: 8 MPI processes
  type: ksponly
  maximum iterations=50, maximum function evaluations=10000
  tolerances: relative=1e-08, absolute=1e-50, solution=1e-08
  total number of linear solver iterations=47
  total number of function evaluations=2
  norm schedule ALWAYS
  SNESLineSearch Object:   8 MPI processes
    type: basic
    maxstep=1.000000e+08, minlambda=1.000000e-12
    tolerances: relative=1.000000e-08, absolute=1.000000e-15, lambda=1.000000e-08
    maximum iterations=1
  KSP Object:   8 MPI processes
    type: fgmres
      GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
      GMRES: happy breakdown tolerance 1e-30
    maximum iterations=10000
    tolerances:  relative=0.001, absolute=1e-50, divergence=10000
    right preconditioning
    using nonzero initial guess
    using UNPRECONDITIONED norm type for convergence test
  PC Object:   8 MPI processes
    type: fieldsplit
      FieldSplit with Schur preconditioner, factorization UPPER
      Preconditioner for the Schur complement formed from A11
      Split info:
      Split number 0 Defined by IS
      Split number 1 Defined by IS
      KSP solver for A00 block
        KSP Object:        (fieldsplit_u_)         8 MPI processes
          type: fgmres
            GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
            GMRES: happy breakdown tolerance 1e-30
          maximum iterations=2, initial guess is zero
          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
          right preconditioning
          using UNPRECONDITIONED norm type for convergence test
        PC Object:        (fieldsplit_u_)         8 MPI processes
          type: mg
            MG: type is MULTIPLICATIVE, levels=3 cycles=v
              Cycles per PCApply=1
              Not using Galerkin computed coarse grid matrices
          Coarse grid solver -- level -------------------------------
            KSP Object:            (fieldsplit_u_mg_coarse_)             8 MPI processes
              type: gmres
                GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                GMRES: happy breakdown tolerance 1e-30
              maximum iterations=1, initial guess is zero
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_coarse_)             8 MPI processes
              type: bjacobi
                block Jacobi: number of blocks = 8
                Local solve is same for all blocks, in the following KSP and PC objects:
              KSP Object:              (fieldsplit_u_mg_coarse_sub_)               1 MPI processes
                type: preonly
                maximum iterations=10000, initial guess is zero
                tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                left preconditioning
                using NONE norm type for convergence test
              PC Object:              (fieldsplit_u_mg_coarse_sub_)               1 MPI processes
                type: ilu
                  ILU: out-of-place factorization
                  0 levels of fill
                  tolerance for zero pivot 2.22045e-14
                  matrix ordering: natural
                  factor fill ratio given 1, needed 1
                    Factored matrix follows:
                      Mat Object:                       1 MPI processes
                        type: seqaij
                        rows=243, cols=243, bs=3
                        package used to perform factorization: petsc
                        total: nonzeros=28431, allocated nonzeros=28431
                        total number of mallocs used during MatSetValues calls =0
                          using I-node routines: found 54 nodes, limit used is 5
                linear system matrix = precond matrix:
                Mat Object:                 1 MPI processes
                  type: seqaij
                  rows=243, cols=243, bs=3
                  total: nonzeros=28431, allocated nonzeros=28431
                  total number of mallocs used during MatSetValues calls =0
                    using I-node routines: found 54 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object:               8 MPI processes
                type: mpiaij
                rows=1275, cols=1275, bs=3
                total: nonzeros=256671, allocated nonzeros=256671
                total number of mallocs used during MatSetValues calls =0
                  has attached near null space
                  using I-node (on process 0) routines: found 54 nodes, limit used is 5
          Down solver (pre-smoother) on level 1 -------------------------------
            KSP Object:            (fieldsplit_u_mg_levels_1_)             8 MPI processes
              type: gmres
                GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                GMRES: happy breakdown tolerance 1e-30
              maximum iterations=2
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using nonzero initial guess
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_levels_1_)             8 MPI processes
              type: bjacobi
                block Jacobi: number of blocks = 8
                Local solve is same for all blocks, in the following KSP and PC objects:
              KSP Object:              (fieldsplit_u_mg_levels_1_sub_)               1 MPI processes
                type: preonly
                maximum iterations=10000, initial guess is zero
                tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                left preconditioning
                using NONE norm type for convergence test
              PC Object:              (fieldsplit_u_mg_levels_1_sub_)               1 MPI processes
                type: ilu
                  ILU: out-of-place factorization
                  0 levels of fill
                  tolerance for zero pivot 2.22045e-14
                  matrix ordering: natural
                  factor fill ratio given 1, needed 1
                    Factored matrix follows:
                      Mat Object:                       1 MPI processes
                        type: seqaij
                        rows=1275, cols=1275, bs=3
                        package used to perform factorization: petsc
                        total: nonzeros=256671, allocated nonzeros=256671
                        total number of mallocs used during MatSetValues calls =0
                          using I-node routines: found 425 nodes, limit used is 5
                linear system matrix = precond matrix:
                Mat Object:                (Buu_)                 1 MPI processes
                  type: seqaij
                  rows=1275, cols=1275, bs=3
                  total: nonzeros=256671, allocated nonzeros=256671
                  total number of mallocs used during MatSetValues calls =0
                    using I-node routines: found 425 nodes, limit used is 5
              linear system matrix = precond matrix:
              Mat Object:              (Buu_)               8 MPI processes
                type: mpiaij
                rows=8019, cols=8019, bs=3
                total: nonzeros=2.17655e+06, allocated nonzeros=2.17655e+06
                total number of mallocs used during MatSetValues calls =0
                  has attached near null space
          Up solver (post-smoother) same as down solver (pre-smoother)
          Down solver (pre-smoother) on level 2 -------------------------------
            KSP Object:            (fieldsplit_u_mg_levels_2_)             8 MPI processes
              type: chebyshev
                Chebyshev: eigenvalue estimates:  min = 0.496672, max = 2.73169
                Chebyshev: eigenvalues estimated using gmres with translations  [0 0.2; 0 1.1]
                KSP Object:                (fieldsplit_u_mg_levels_2_esteig_)                 8 MPI processes
                  type: gmres
                    GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                    GMRES: happy breakdown tolerance 1e-30
                  maximum iterations=10, initial guess is zero
                  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                  left preconditioning
                  using NONE norm type for convergence test
              maximum iterations=4
              tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
              left preconditioning
              using nonzero initial guess
              using NONE norm type for convergence test
            PC Object:            (fieldsplit_u_mg_levels_2_)             8 MPI processes
              type: jacobi
              linear system matrix = precond matrix:
              Mat Object:              (fieldsplit_u_)               8 MPI processes
                type: shell
                rows=107811, cols=107811, bs=3
          Up solver (post-smoother) same as down solver (pre-smoother)
          linear system matrix = precond matrix:
          Mat Object:          (fieldsplit_u_)           8 MPI processes
            type: shell
            rows=107811, cols=107811, bs=3
      KSP solver for S = A11 - A10 inv(A00) A01 
        KSP Object:        (fieldsplit_p_)         8 MPI processes
          type: preonly
          maximum iterations=10000, initial guess is zero
          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
          left preconditioning
          using NONE norm type for convergence test
        PC Object:        (fieldsplit_p_)         8 MPI processes
          type: jacobi
          linear system matrix followed by preconditioner matrix:
          Mat Object:          (fieldsplit_p_)           8 MPI processes
            type: schurcomplement
            rows=16384, cols=16384
              Schur complement A11 - A10 inv(A00) A01
              A11
                Mat Object:                (fieldsplit_p_)                 8 MPI processes
                  type: mpisbaij
                  rows=16384, cols=16384, bs=4
                  total: nonzeros=65536, allocated nonzeros=65536
                  total number of mallocs used during MatSetValues calls =0
              A10
                Mat Object:                (Bpu_)                 8 MPI processes
                  type: shell
                  rows=16384, cols=107811
              KSP of A00
                KSP Object:                (fieldsplit_u_)                 8 MPI processes
                  type: fgmres
                    GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                    GMRES: happy breakdown tolerance 1e-30
                  maximum iterations=2, initial guess is zero
                  tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                  right preconditioning
                  using UNPRECONDITIONED norm type for convergence test
                PC Object:                (fieldsplit_u_)                 8 MPI processes
                  type: mg
                    MG: type is MULTIPLICATIVE, levels=3 cycles=v
                      Cycles per PCApply=1
                      Not using Galerkin computed coarse grid matrices
                  Coarse grid solver -- level -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_coarse_)                     8 MPI processes
                      type: gmres
                        GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                        GMRES: happy breakdown tolerance 1e-30
                      maximum iterations=1, initial guess is zero
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_coarse_)                     8 MPI processes
                      type: bjacobi
                        block Jacobi: number of blocks = 8
                        Local solve is same for all blocks, in the following KSP and PC objects:
                      KSP Object:                      (fieldsplit_u_mg_coarse_sub_)                       1 MPI processes
                        type: preonly
                        maximum iterations=10000, initial guess is zero
                        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                        left preconditioning
                        using NONE norm type for convergence test
                      PC Object:                      (fieldsplit_u_mg_coarse_sub_)                       1 MPI processes
                        type: ilu
                          ILU: out-of-place factorization
                          0 levels of fill
                          tolerance for zero pivot 2.22045e-14
                          matrix ordering: natural
                          factor fill ratio given 1, needed 1
                            Factored matrix follows:
                              Mat Object:                               1 MPI processes
                                type: seqaij
                                rows=243, cols=243, bs=3
                                package used to perform factorization: petsc
                                total: nonzeros=28431, allocated nonzeros=28431
                                total number of mallocs used during MatSetValues calls =0
                                  using I-node routines: found 54 nodes, limit used is 5
                        linear system matrix = precond matrix:
                        Mat Object:                         1 MPI processes
                          type: seqaij
                          rows=243, cols=243, bs=3
                          total: nonzeros=28431, allocated nonzeros=28431
                          total number of mallocs used during MatSetValues calls =0
                            using I-node routines: found 54 nodes, limit used is 5
                      linear system matrix = precond matrix:
                      Mat Object:                       8 MPI processes
                        type: mpiaij
                        rows=1275, cols=1275, bs=3
                        total: nonzeros=256671, allocated nonzeros=256671
                        total number of mallocs used during MatSetValues calls =0
                          has attached near null space
                          using I-node (on process 0) routines: found 54 nodes, limit used is 5
                  Down solver (pre-smoother) on level 1 -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_levels_1_)                     8 MPI processes
                      type: gmres
                        GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                        GMRES: happy breakdown tolerance 1e-30
                      maximum iterations=2
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using nonzero initial guess
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_levels_1_)                     8 MPI processes
                      type: bjacobi
                        block Jacobi: number of blocks = 8
                        Local solve is same for all blocks, in the following KSP and PC objects:
                      KSP Object:                      (fieldsplit_u_mg_levels_1_sub_)                       1 MPI processes
                        type: preonly
                        maximum iterations=10000, initial guess is zero
                        tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                        left preconditioning
                        using NONE norm type for convergence test
                      PC Object:                      (fieldsplit_u_mg_levels_1_sub_)                       1 MPI processes
                        type: ilu
                          ILU: out-of-place factorization
                          0 levels of fill
                          tolerance for zero pivot 2.22045e-14
                          matrix ordering: natural
                          factor fill ratio given 1, needed 1
                            Factored matrix follows:
                              Mat Object:                               1 MPI processes
                                type: seqaij
                                rows=1275, cols=1275, bs=3
                                package used to perform factorization: petsc
                                total: nonzeros=256671, allocated nonzeros=256671
                                total number of mallocs used during MatSetValues calls =0
                                  using I-node routines: found 425 nodes, limit used is 5
                        linear system matrix = precond matrix:
                        Mat Object:                        (Buu_)                         1 MPI processes
                          type: seqaij
                          rows=1275, cols=1275, bs=3
                          total: nonzeros=256671, allocated nonzeros=256671
                          total number of mallocs used during MatSetValues calls =0
                            using I-node routines: found 425 nodes, limit used is 5
                      linear system matrix = precond matrix:
                      Mat Object:                      (Buu_)                       8 MPI processes
                        type: mpiaij
                        rows=8019, cols=8019, bs=3
                        total: nonzeros=2.17655e+06, allocated nonzeros=2.17655e+06
                        total number of mallocs used during MatSetValues calls =0
                          has attached near null space
                  Up solver (post-smoother) same as down solver (pre-smoother)
                  Down solver (pre-smoother) on level 2 -------------------------------
                    KSP Object:                    (fieldsplit_u_mg_levels_2_)                     8 MPI processes
                      type: chebyshev
                        Chebyshev: eigenvalue estimates:  min = 0.496672, max = 2.73169
                        Chebyshev: eigenvalues estimated using gmres with translations  [0 0.2; 0 1.1]
                        KSP Object:                        (fieldsplit_u_mg_levels_2_esteig_)                         8 MPI processes
                          type: gmres
                            GMRES: restart=30, using Classical (unmodified) Gram-Schmidt Orthogonalization with no iterative refinement
                            GMRES: happy breakdown tolerance 1e-30
                          maximum iterations=10, initial guess is zero
                          tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                          left preconditioning
                          using NONE norm type for convergence test
                      maximum iterations=4
                      tolerances:  relative=1e-05, absolute=1e-50, divergence=10000
                      left preconditioning
                      using nonzero initial guess
                      using NONE norm type for convergence test
                    PC Object:                    (fieldsplit_u_mg_levels_2_)                     8 MPI processes
                      type: jacobi
                      linear system matrix = precond matrix:
                      Mat Object:                      (fieldsplit_u_)                       8 MPI processes
                        type: shell
                        rows=107811, cols=107811, bs=3
                  Up solver (post-smoother) same as down solver (pre-smoother)
                  linear system matrix = precond matrix:
                  Mat Object:                  (fieldsplit_u_)                   8 MPI processes
                    type: shell
                    rows=107811, cols=107811, bs=3
              A01
                Mat Object:                (Bup_)                 8 MPI processes
                  type: shell
                  rows=107811, cols=16384
          Mat Object:          (fieldsplit_p_)           8 MPI processes
            type: mpisbaij
            rows=16384, cols=16384, bs=4
            total: nonzeros=65536, allocated nonzeros=65536
            total number of mallocs used during MatSetValues calls =0
    linear system matrix followed by preconditioner matrix:
    Mat Object:    (stokes_Amf_)     8 MPI processes
      type: shell
      rows=124195, cols=124195
    Mat Object:     8 MPI processes
      type: nest
      rows=124195, cols=124195
        Matrix object: 
          type=nest, rows=2, cols=2 
          MatNest structure: 
          (0,0) : prefix="fieldsplit_u_", type=shell, rows=107811, cols=107811 
          (0,1) : prefix="Bup_", type=shell, rows=107811, cols=16384 
          (1,0) : prefix="Bpu_", type=shell, rows=16384, cols=107811 
          (1,1) : prefix="fieldsplit_p_", type=mpisbaij, rows=16384, cols=16384 
Update rheology (viscous) [mpoint]: (min,max)_eta 1.00e-03,1.00e+00; log10(max/min) 3.00e+00; cpu time 1.29e-03 (sec)
[[ModelOutput_ViscousSinker]]
[[DESIGN FLAW]] pTatin3d_ModelOutput_VelocityPressure_Stokes: require better physics modularity to extract (u,p) <---| (X) 
[[DESIGN FLAW]] pTatinOutputMeshVelocityPressureVTS_v0_binary: only printing P0 component of pressure field 
pTatin3d_ModelOutput_VelocityPressure_Stokes() -> step000000_vp.(pvd,pvts,vts): CPU time 3.78e-02 (sec) 
pTatin3d_ModelOutput_MPntStd() -> step000000_mpoints_std.(pvd,pvtu,vtu): CPU time 2.09e-02 (sec) 
  TimeStep control(StkCourant): | current = 1.0000e+30 : trial = 2.2528e-02 [accepted] | ==>> dt used = 2.2528e-02 |
  TimeStep control(StkSurfaceCourant): | current = 2.2528e-02 : trial = 5.9387e+31 | ==>> dt used = 2.2528e-02 |
  timestep[] dt_courant = 2.2528e-02 
